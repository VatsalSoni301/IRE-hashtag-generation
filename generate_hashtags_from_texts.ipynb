{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/vatsal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/vatsal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "from nltk.corpus import wordnet\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "import os\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove import Corpus, Glove\n",
    "corpus = Corpus() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_corpora = []\n",
    "top100 = ['#travel', '#wanderlust', '#nature', '#travelling', '#traveling', '#traveller', '#photography', '#traveler', '#trip', '#travels', '#vacation', '#love', '#travelers', '#adventure', '#tourist', '#landscape', '#travellers', '#holiday', '#explore', '#beautiful', '#tourism', '#hiking', '#beach', '#photo', '#sunset', '#photographer', '#mountains', '#globetrotter', '#summer', '#art', '#sky', '#treking', '#europe', '#view', '#architecture', '#sea', '#fun', '#happy', '#city', '#sun', '#amazing', '#lifestyle', '#backpacking', '#wanderer', '#italy', '#follow', '#life', '#visiting', '#fashion', '#autumn', '#ocean', '#outdoors', '#explorer', '#world', '#india', '#beauty', '#mountain', '#spain', '#style', '#backpacker', '#like', '#clouds', '#france', '#exploring', '#trekking', '#asia', '#me', '#friends', '#canon', '#usa', '#blogger', '#ig', '#happiness', '#sunrise', '#smile', '#holidays', '#girl', '#wander', '#germany', '#island', '#paradise', '#Travel', '#flowers', '#discover', '#voyage', '#turkey', '#sightseeing', '#landscapes', '#italia', '#outdoor', '#cute', '#indonesia', '#history', '#food', '#pic', '#forest', '#beaches', '#inspiration', '#green', '#memories']\n",
    "top100 = [r.replace('#', '') for r in top100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_stemmed = [ps.stem(word) for word in top100]\n",
    "top100_mapping = {}\n",
    "for i in range(0,100):\n",
    "    top100_mapping[top100_stemmed[i]]=top100[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punc(s):\n",
    "    new_str = \"\"\n",
    "    for c in s:\n",
    "        if c in punctuation:\n",
    "            new_str += \" \"\n",
    "        else:\n",
    "            new_str += c\n",
    "    return new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./jsons/Luxarytravel.json\n",
      "./jsons/travelbook.json\n",
      "./jsons/Traveldeeper.json\n",
      "./jsons/Hiking.json\n",
      "./jsons/travelquotes.json\n",
      "./jsons/travelstoke.json\n",
      "./jsons/travel.json\n",
      "./jsons/traveladdict.json\n",
      "./jsons/travellersnotebook.json\n",
      "./jsons/travelguide.json\n",
      "./jsons/Travelabout.json\n",
      "./jsons/trip.json\n",
      "./jsons/solotravel.json\n",
      "./jsons/Travelphotography.json\n",
      "./jsons/travelbug.json\n",
      "./jsons/travelpic.json\n",
      "./jsons/travelgram.json\n",
      "./jsons/tourist.json\n",
      "./jsons/travelislife.json\n",
      "./jsons/beachvibes.json\n",
      "./jsons/treking.json\n",
      "./jsons/Travelawesome.json\n",
      "./jsons/traveltheglobe.json\n",
      "./jsons/travelworld.json\n",
      "./jsons/worldtraveller.json\n",
      "./jsons/Travellove.json\n",
      "./jsons/nature.json\n",
      "./jsons/travelcaptures.json\n",
      "./jsons/Citytravel.json\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(\"./jsons/\"):\n",
    "    print(\"./jsons/\" + file)\n",
    "    file_ptr = open(\"./jsons/\" + file, \"r\")\n",
    "    dic = json.load(file_ptr)\n",
    "    for post in dic:\n",
    "        text = strip_punc(dic[post]['text_des'].lower())\n",
    "        toks = word_tokenize(text)\n",
    "        toks_ = []\n",
    "        for tok in toks:\n",
    "            if not wordnet.synsets(tok) or len(tok) < 3:\n",
    "                continue\n",
    "            tok = ps.stem(tok)\n",
    "            if tok not in stop_words:\n",
    "                toks_.append(tok)\n",
    "        if toks_:\n",
    "            glob_corpora.append(toks_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25481\n"
     ]
    }
   ],
   "source": [
    "print(len(glob_corpora))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using word2vec for training\n",
    "\n",
    "glob_model = Word2Vec(glob_corpora, min_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('monsoon', 0.9906405210494995), ('leaf', 0.9840993881225586), ('snowi', 0.9808374047279358), ('dramat', 0.9807693362236023), ('butterfli', 0.9797319173812866), ('mist', 0.9773508310317993), ('plant', 0.9765117168426514), ('mane', 0.9760971069335938), ('fog', 0.9753571152687073), ('lite', 0.9749687910079956), ('lush', 0.9747658371925354), ('baum', 0.9730554223060608), ('dusk', 0.9719358682632446), ('branch', 0.9707391262054443), ('foggi', 0.9700635671615601), ('bamboo', 0.9697462916374207), ('wood', 0.9692462682723999), ('stream', 0.968997597694397), ('clad', 0.9688808917999268), ('redwood', 0.9674018621444702)]\n"
     ]
    }
   ],
   "source": [
    "print(glob_model.wv.most_similar(positive = 'grass', topn = 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 30 training epochs with 4 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n"
     ]
    }
   ],
   "source": [
    "# Using Glove for training\n",
    "\n",
    "corpus.fit(glob_corpora, window = 10)\n",
    "glove = Glove(no_components = 100, learning_rate = 0.05)\n",
    "glove.fit(corpus.matrix, epochs = 30, no_threads = 4, verbose = True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "glove.save('glove.model')\n",
    "glove.add_dictionary(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enjoy', 'easi', 'jog', 'mountain', 'forest', 'thought', 'escap', 'world', 'bit', 'travel']\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing of input text\n",
    "\n",
    "input_sentence = \"#forest#green#wanderlust#tour\"\n",
    "input_sentence = \"I enjoy an easy jog. In the mountains, or the forest with my own thoughts, I can escape from the world for a bit. #travel#travellife\"\n",
    "text = strip_punc(input_sentence.lower())\n",
    "toks = word_tokenize(text)\n",
    "toks_ = []\n",
    "for tok in toks:\n",
    "    if wordnet.synsets(tok):\n",
    "        tok = ps.stem(tok)\n",
    "        if tok not in stop_words and len(tok)>2:\n",
    "            toks_.append(tok)\n",
    "\n",
    "input_words = toks_\n",
    "print(input_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "(10, 100)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# Calculating sentence embedding using word2vec\n",
    "\n",
    "sent = []\n",
    "for word in input_words:\n",
    "    if word in glob_model.wv.vocab:\n",
    "        sent.append(glob_model.wv[word])\n",
    "    \n",
    "print(len(sent))\n",
    "sent = np.array(sent)\n",
    "print(sent.shape)\n",
    "embed = np.average(sent, axis = 0)\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.8708082, 'ig'), (0.80178446, 'beauty'), (0.7823005, 'love'), (0.7746443, 'adventure'), (0.77142674, 'exploring'), (0.76981014, 'world'), (0.7670982, 'like'), (0.76384693, 'nature'), (0.7550086, 'green'), (0.75313425, 'view'), (0.7530867, 'inspiration'), (0.74956316, 'life'), (0.7438802, 'amazing'), (0.74356675, 'outdoor'), (0.7383999, 'landscapes'), (0.7364143, 'memories'), (0.72931117, 'mountain'), (0.72159785, 'wanderlust'), (0.71721, 'forest'), (0.7161728, 'friends')]\n"
     ]
    }
   ],
   "source": [
    "# Finding similarity of top 100 hashtags using word2vec\n",
    "\n",
    "ans = []\n",
    "for hasht in top100_mapping:\n",
    "    try:\n",
    "        v1 = glob_model.wv[hasht]\n",
    "        simi = np.dot(v1, embed) / (np.linalg.norm(v1) * np.linalg.norm(embed))\n",
    "        ans.append((simi, top100_mapping[hasht]))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "ans = sorted(ans, reverse = True)\n",
    "print(ans[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ig', 'beauty', 'love', 'adventure', 'exploring', 'world', 'like', 'nature', 'green', 'view']\n"
     ]
    }
   ],
   "source": [
    "# Finally top k for word2vec\n",
    "\n",
    "k = 10\n",
    "topk_word2vec = [h[1] for h in ans[:k]]\n",
    "print(topk_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['forest', 'green', 'wanderlust', 'tour']\n",
      "(4, 100)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# Calculating sentence embedding using glove\n",
    "\n",
    "sent = []\n",
    "print(input_words)\n",
    "for word in input_words:\n",
    "    if word in glove.dictionary:\n",
    "        sent.append(glove.word_vectors[glove.dictionary[word]])\n",
    "    \n",
    "sent = np.array(sent)\n",
    "print(sent.shape)\n",
    "embed = np.average(sent, axis = 0)\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.8380648704679269, 'wanderlust'), (0.8145650633528564, 'adventure'), (0.8074920271173722, 'green'), (0.8073567039677635, 'wander'), (0.796642199847806, 'outdoor'), (0.7964946952104364, 'exploring'), (0.7501473393295741, 'mountain'), (0.7222836597743094, 'hiking'), (0.7202703288855951, 'forest'), (0.6950989619387951, 'photography'), (0.6895518054964919, 'landscapes'), (0.6802330189692215, 'trip'), (0.6801947216846282, 'backpacker'), (0.6774100972886504, 'nature'), (0.676163261342647, 'trekking'), (0.6698956183488703, 'tourist'), (0.6670689142296702, 'flowers'), (0.6656441327919346, 'beauty'), (0.6638979517838771, 'sky'), (0.6500933784545571, 'vacation')]\n"
     ]
    }
   ],
   "source": [
    "# Finding similarity using glove embeddings\n",
    "\n",
    "ans2 = []\n",
    "for hasht in top100_mapping:\n",
    "    try:\n",
    "        v1 = glove.word_vectors[glove.dictionary[hasht]]\n",
    "        simi = np.dot(v1, embed) / (np.linalg.norm(v1) * np.linalg.norm(embed))\n",
    "        ans2.append((simi, top100_mapping[hasht]))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "ans2 = sorted(ans2, reverse = True)\n",
    "print(ans2[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wanderlust', 'adventure', 'green', 'wander', 'outdoor', 'exploring', 'mountain', 'hiking', 'forest', 'photography']\n"
     ]
    }
   ],
   "source": [
    "# Finally topk for glove\n",
    "k = 10\n",
    "\n",
    "topk_glove = [h[1] for h in ans2[:k]]\n",
    "print(topk_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wanderlust', 'adventure', 'green', 'wander', 'outdoor', 'exploring', 'mountain', 'hiking', 'forest', 'photography'] ['outdoor', 'wanderlust', 'usa', 'sightseeing', 'exploring', 'ig', 'backpacker', 'wander', 'landscapes', 'asia']\n"
     ]
    }
   ],
   "source": [
    "print(topk_glove, topk_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
